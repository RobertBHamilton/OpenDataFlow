#!/bin/bash  
#
# Look for an environment registered in DataFlow, if a suitable data chunk is available then
# Set the environment and execute the command in a new process
# If not then drop a message and exit 

# Added to make cronjobs easier to set CLASSPATH
thisdir=`dirname ${BASH_SOURCE[0]}`
cd $thisdir

# support alternate way to pass the key 
if [ -z "$PASSKEY" ];then
  export passkey=$1
  shift
else 
  export passkey=$PASSKEY
fi

# Why not just parse the jobid from command
export cmd=$1
export jobid=`basename $cmd`   #strip off the directory path
jobid=${jobid%.*}              #strip off the extention
shift 1
export args="$@"
# 
#export CLASSPATH=bin/postgresql-42.7.3.jar:bin/json-20250517.jar:utility/target/utility-1.0.0.jar
#export CLASSPATH=bin/json-20250517.jar:bin/h2-2.2.224.jar:utility/target/utility-1.0.0.jar
# get the jar file either in current directory or in canonical build location
jarfile="`ls  utility/target/dataflow*jar dataflow*jar 2>/dev/null|tail -1`"
if [ -f "$jarfile" ];then
        export jarc="java -jar $jarfile $PASSKEY "
else
        echo we need the dataflow-version.jar to run this utility. Cannot continue
        exit
fi
export CLASSPATH=$jarfile


getEnvJSON(){
  #java com.hamiltonlabs.dataflow.utility.GetJobData $jobid $passkey 
  java com.hamiltonlabs.dataflow.utility.LaunchJob $passkey $jobid 
}

decrypt() {
java com.hamiltonlabs.dataflow.utility.Cryptor -d $passkey "$1"
}

# Because we pipe to the parse, it creates a separate process and so any changes we make
# to the environment will vanish when the process exits. Instead we just pipe it to source 
# using Bash process substitution
declareEnv(){
  #stale dataid will result in false executions so make sure we only set it here
  unset dataid   
  getEnvJSON|tail -1|jq -c '.[] | to_entries[]' | while read -r entry; do
 

  key=$(echo "$entry" | jq -r '.key')
  value=$(echo "$entry" | jq -r '.value')

# FIXME there is some bug when value contains a path symbol */
  if [[ $key =~ ^today.* ]];then
     export a=$a;	#do nothing dont pollute the env with the automatic dataset */
  else    
    # Emit export statement
    echo "declare -x $key=\"${value}\";"


    # Handle encrypted passwords
    if [[ $key =~ ^(.*)_encryptedpass$ ]]; then
      prefix="${BASH_REMATCH[1]}"
      decrypted=$(decrypt "$value")
      echo "declare -x ${prefix}_password=\"${decrypted}\";"
    fi
  fi
done
}

# test
env="`declareEnv`"
# uncomment if you need to debug echo "$env"

source <(echo "$env")
if [ -z "$dataid" ];then
  echo "`date`: no suitable data available for job. Not running it"
else

  echo "`date`: Launching $cmd with dataid $dataid"
  eval "$cmd $args"
  if [ $? -eq 0 ];then
     echo "`date`: Job $cmd is complete. Updating status"
     java -cp $CLASSPATH:app.jar com.hamiltonlabs.dataflow.utility.SetJobEndStatus $passkey $jobid $dataid READY
  else
     echo "`date`: Job $cmd has failed. Updating status"
     java -cp $CLASSPATH:app.jar com.hamiltonlabs.dataflow.utility.SetJobEndStatus $passkey $jobid $dataid FAILED
  fi
fi
