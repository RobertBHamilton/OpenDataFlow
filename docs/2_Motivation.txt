# OpenDataFlow.core

History.

1. Many Years ago, I was holding a devops pager for the entire datawarehouse feed for a major corporation. When a job failed the devops guy was supposed to root cause the failure, make whatever cleanups were needed, restart the job, note successful completiion and communicate downstream the resolution.

   One weekend because of a network misconfiguration deployed on Friday, all 700 jobs running each running on one of a half dozen possible servers, in the daily batch cycle failed.  File paths for either extracts or loads were hardcoded in scripts, as will as connection strings to the databases either extracted or loaded, logs were not systematically saved, and filesystem flags  were the only clue as to the final status of any job. 

Due dilligence requires the basic questions need to be answered: was the data really ready when the job ran, was it running on the right data (the expected sequence number), was it in the physical location expected (correct environment,fileserver and path?), was it validated, was it safe to access at the time? Even that basic preliminary root cause analysis involved locating the specific flag files, extracting the job sequence number, then diving into the code to find location of data sets and then examining the data set to determine the issue, a process which was tedious, time consuming and error prone.

After that mess was fixed, the natural postmortem thought is, wouldn't life be simpler if we actually captured the information necessary to answer those questions? What if we didn't need tribal knowledge to erase or to to create file system flags and restart in the exact order needed?  

That lesson is what motivated the principles that are built into this framework.


2.  A few years after that, I took it upon myself to extract a large data store from a relational database and load it into a Big Data platform.  It was roughly 100TB in size but would be manageable if sharded into 10,000 extract jobs of 10GB each followed by load jobs into the Big Data.  If each job was 20 minutes extract and 20 minutes load then the migration would be 7000 hours total. So it would be prudent to enable 10x concurrency and complete the task in 4 weeks, but be able to scale up and down the concurrency at will because others would be using the production servers at times. But even so, it is no fun to keep eyes on glass for 700 hours. The orchestration has to be very robust. A 1% job failure rate is quite possible on Hadoop mapreduce so restartability if also a priority.
It turns out that paying attention to the five questions (ready?right?location?valid?safe?) and keeping the answers into a data store was sufficient to make this into a fire-and-forget script.  

3. In the years following, I had other opportunities to do large scale data migration/massive batch jobs. I typically just unpacked my dataflow scripts, configured them for the current jobs and let them run. I'd check in every few days and restart any that failed, but there was little more than 5 minutes a day to manage it.

4. The shell script implementation and local storage for data status was fine for personal use and any one-off months long task, and it impressed executive leadership nicely, but it was not so good for handing off to others who may not be so comfortable with services written purely in scripting language.
So I've decided to take the lessons learned and apply them to a framework written from scratch, but now using enterprise-class data store and service.
This one, using entirely new code, can be shared as an open source utiility.

The goal is that an ETL developer could download and set up the framework in about one day, configure it for their particular jobs in another day, and be running massive partitioned job streams by day 3.





-------------------------------------------------------------------------
Here is a deeper perspective:

Taking the viewpoint of an ETL resource, I want my script or process to be supplied all the input data (including all connection data), the location of the output data, any job sequence number, and to be guaranteed that I'm operating on the data sets required by business requirements, with no chance of gaps, overlaps, or undetected misses due to error states.  That keeps the ETL script simple as the most complex parts of the  logic is handled by the framework. It keeps the script simple and generic.

If a job has failed, I also want to be able to restart it with no special cleanups, just set a field to "resubmit" and let the normal flow continue.
If there are future questions about data integrity, I want to be able to answer the five questions quickly and easily (exception to Brandolini principle) based on status information saved in the dataflow datastore.

Our perspective is that such a workflow needs to have at least the following features:

1. capability to partition the data into chunks which can be independently processed, and the framework should associate a unique identifier to each chunk in order to track its status.  The undivided data set is called DataSet, the discrete partitions of it are called DataChunks.

2. Status of every data chunk is maintained and available to the ETL jobs. This status includes whether it is ready to consume, or still being produced, complete, or error or (sometimes important) being consumed.

3. Job Status. every job is associated with 1 or more data chunks input and produces one or more data chunks output. In practice, job status (running,failed,complete,validated) is the same as the status of its output data, so job status is denormalized into data status table. More specifically since every data set is uniquely produced as output of some job run, jobID (name of script for example) + dataID+datasetID serves to uniquely identify the job producing that data chunk.

4. transaction model. The data is to be transformed and forwarded downstream, the handling of it requires the kind of robustness that is usually supplied via transaction semantics.

5. Though not strictly necessary, jobs may require additional metadata, which can be constructed by the framework and supplied to them when they are started.


This is the core component of OpenDataFlow utilities. It handles all accesses to the underlying data store of the framework. 

OpenDataFlow treats the data handoff between ETL processes in a transactional model. The handshake requires certain guarantees made by the producer in order that the consumer may accept the data. This is a data centric model in that these guarantees involve characteristics or status of the data being handed off. It is a departure from traditional job schedulers which are typically process centric instead. Where the traditional schedulers make assumptions about the data status, DataFlow instead specifically tracks the status and permits processes to rely on reliable data status.

The DataFlow core components manage three sets of information.

1. Real time status of all data sets tracked by the tool
2. Metadata for all tracked data sources
3. Metadata for fully registered jobs



# History and acknowledgements.
The concepts and design pattern concepts that I have been evolving and refining over several decades during my career as an ETL software engineer. It arises from career experience that spans several fortune 50 companies and a few smaller ones.  This design pattern is designed specifically to address the issues and headaches encountered during this time while performing tasks in involing erprise scale data movement. We are startng this program from scratch, for the first time designing a framework from the ground up.  It a core framework modeled on concepts which have accumulated over the years with experience and learning. It is also a reference implementation built on the framework. The reference framework itself will be fully functional tools useful for enterprise class applications.

# Motivation

After doing root cause analysis, impact analysis, tracing data lineage and other data forensics, the questions often asked are "was this task more tedious and less reliable than it should be?" "Are there ways to speed up the investigation and make it more reliable?"  
Once the outage has been root caused, the question that follows is "What can be done to reduce the occurence of outage? Once failure modes are identified and root caused the natural question is can more be done to reduce the likelihood of the outage to occure again? Can we minimize factors that contribute to the root cause.  
While recovoring from the outage, are there manual steps involved in restarting processes, determining if data dependencies are met, cleaning up stale files or flags?
Can we reduce the amount of specialized knowledge needed to restart the entire flow, and reduce the complexity of the tasks.
 
The answer to all these questions is most certainly "yes".  However, traditional job schedulers however sophisticated they are, may not provide the kind of functionality needed to deal with these specific questions. As evidence of that, I would cite many incidents in which the principle investigator manually performs all of data mining, root cause and correction, manual cleanups and data dependency analysis, and when each job is ready for a relatively clean start they then ask scheduling services to restart failed jobs, possibly one by one.

These tend to be expensive exercises. The more that we can automate the capture of useful information at runtime, automate the dependency tracking, automate or systematize the logic, and make job starts/restarts nearly touchless, the more reliable the enterprise data/analytics platform will become.

In this author's experience over nearly three decades of ETL developement and support, almost all operational issues with data flow come about because of inaccurate assumptions made with respect to these 5 questions:

# The five questions supporting the transactional model


Our approach is to treat each data set consumed or produced by a job as an indivisible unit, and one that is passed from output of one job to input of the next with the same kind of checks and controls that you would apply to any data transfer protocol.

Simply put, in order for a job to accept a chunk of data for consumption, it needs to confirm the answer "yes" to each of these five questions.

1. Is the data ready
2. Is it the correct data
3. Is the data in the physical location the consumer expects
4. Is the data validated
5. Is the data safe to access or modify

All data transfer protocols are built around these questions one way or another. It is quite reasonable to posit that ETL building on larger blocks of data have comparable requirements.  These are also the first questions the analyst must answer when tasked with finding root cause of job failures. 

Both the task of the analyst and the logic used by the consumer are simplified if this information is systematically captured at run time and directly used for coordination between jobs having data dependencies.



